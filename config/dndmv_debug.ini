[hyperparams]
dmv_mode = "tdr"
nn_mode = "tdr"

num_lex = 0

cv = 2
e_step_mode = "viterbi"
count_smoothing = 1e-1
param_smoothing = 1e-1

dim_pos_emb = 20
dim_word_emb = 1536
dim_valence_emb = 20
dim_deprel_emb = 10
dim_relation_emb = 10
dim_hidden = 200

dim_pre_out_decision = 32
dim_pre_out_child = 64
dim_pre_out_root = 64

lstm_dim_in = 100
lstm_dim_out = 16
lstm_dropout = 0.
lstm_layers = 1
lstm_bidirectional = True

activation_func = "relu"
dropout = 0.3

freeze_word_emb = False
freeze_pos_emb = True
freeze_out_pos_emb = False

use_emb_as_w = False

end2end = True
lr = 0.001
e_batch_size = 256
m_batch_size = 256
clip_grad = 5.

epoch = 100
epoch_init = 3
epoch_nn = 5
neural_stop_criteria = 1e-3

same_len = False
shuffle = 2
drop_last = False
max_len_train = 60
max_len_eval = 60
min_len_train = 1
num_worker = 3
device = "cuda"

use_pair = False
max_len = 60
initial_tree_sampling = "RB_RB_RB"
encoder = "bert"
kcluster= 30
share_valence_emb=True
pca = True

# [path]
# vocab = "data/bllip_vec/vocab+pos.txt"
# pos = 'data/bllip_vec/pos.txt'

# word_emb = ""
# out_pos_emb = ""
# pos_emb = ""

# pretrained_ds = "data/wsj_han/wsj10_tr_pred"

