[hyperparams]
mlp_dim = 100
weight_decay = 1e-7
optimizer_name = "adam"
n_init_epochs = 3
negative_size = 32

dmv_mode = "tdr"
nn_mode = "tdr"

num_lex = 0

cv = 2
e_step_mode = "viterbi"
count_smoothing = 1e-1
param_smoothing = 1e-1

dim_pos_emb = 20
dim_word_emb = 100
dim_valence_emb = 20
dim_deprel_emb = 10
dim_relation_emb = 10
dim_hidden = 200

dim_pre_out_decision = 32
dim_pre_out_child = 64
dim_pre_out_root = 64

lstm_dim_in = 100
lstm_dim_out = 16
lstm_dropout = 0.
lstm_layers = 1
lstm_bidirectional = True

activation_func = "relu"
dropout = 0.3

freeze_word_emb = False
freeze_pos_emb = False
freeze_out_pos_emb = False

use_emb_as_w = False

end2end = False
lr = 0.001
e_batch_size = 256
m_batch_size = 256
clip_grad = 5.

epoch = 50
epoch_init = 3
epoch_nn = 5
neural_stop_criteria = 1e-3

same_len = False
shuffle = 2
drop_last = False
max_len_train = 1000
max_len_eval = 1000
min_len_train = 1
num_worker = 3
device = "cuda"

use_pair = False
max_len = 1000

[path]
vocab = "data/bllip_vec/vocab+pos.txt"
# pos = 'data/bllip_vec/pos.txt'

word_emb = ""
out_pos_emb = ""
pos_emb = ""

pretrained_ds = "data/wsj_han/wsj10_tr_pred"

