[Data]
pretrained_model = 'bert'
encode_method = 'endpoint'
norm = True
max_length = -1
partition_length = 60
supvised_part = 0.5
unsupervised_weight = 0.0

[Network]
model = 'kmeansbiaffinencrfae'
hidden = 100
dropout = 0.2
layers = 1
act_func = 'tanh'
kcluster = 100
word_embed = True
method = 'head_selection'
coef = 1.0
negative_num = 32
init_epoch = 10

embed_dropout = .33
n_lstm_hidden = 100
n_lstm_layers = 1
lstm_dropout = .33
n_mlp_arc = 100
mlp_dropout = .33
encode_prob = False
identity_biaffine = False

[Optimizer]
lr = 1e-3
mu = .9
nu = .9
epsilon = 1e-12
l2reg = 1e-4
clip = 5.0
decay = 1.0
decay_steps = 5000
smooth = 'uniform'
alpha = 10.0
decoder_learn = False

[Run]
batch_size = 1000
epochs = 200
random_seed = 42